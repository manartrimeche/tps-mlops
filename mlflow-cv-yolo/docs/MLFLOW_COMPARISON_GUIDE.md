# Guide Complet : Comparaison des Runs dans MLflow UI

## √âtape 7 : Comparaison dans l'UI MLflow

Ce guide vous montre comment comparer plusieurs runs d'exp√©riences YOLO dans l'interface MLflow pour analyser les m√©triques et les artefacts.

---

## üéØ Objectif

Comparer les diff√©rentes configurations YOLO (√©poques, taille d'image, learning rate) pour identifier la meilleure configuration bas√©e sur :
- **mAP@50** : Mean Average Precision √† IoU 0.5
- **mAP50-95** : Mean Average Precision moyenn√©e sur IoU 0.5 √† 0.95
- **Precision** : Pr√©cision du mod√®le
- **Recall** : Rappel du mod√®le

---

## üìã √âtapes de Comparaison

### 1. Acc√©der √† l'Exp√©rience

1. Ouvrez votre navigateur et allez sur **http://localhost:5000**
2. Dans la barre lat√©rale gauche, cliquez sur l'exp√©rience **cv_yolo_tiny** (ou Experiment ID 1)
3. Vous verrez la liste des 9 runs g√©n√©r√©s par le script de grid search

### 2. S√©lectionner les Runs √† Comparer

**M√©thode manuelle** (recommand√©e) :

1. **Cliquez sur les checkboxes** √† gauche de chaque run que vous souhaitez comparer
   - S√©lectionnez au moins 2 runs, maximum 10 pour une comparaison claire
   - Exemple : S√©lectionnez 3-4 runs avec diff√©rentes configurations

2. Une fois les runs s√©lectionn√©s :
   - Un bouton **"Compare"** appara√Æt en haut de la table des runs
   - Le nombre de runs s√©lectionn√©s est affich√©

### 3. Cliquer sur "Compare"

1. Cliquez sur le bouton **"Compare"** (g√©n√©ralement en haut √† gauche de la table)
2. Attendez que la page de comparaison se charge
3. Vous serez redirig√© vers : `http://localhost:5000/#/experiments/1/compare-runs/...`

### 4. Analyser la Page de Comparaison

La page de comparaison comporte plusieurs sections :

#### A. **Vue d'ensemble (Overview)**
- Tableau montrant tous les runs s√©lectionn√©s c√¥te √† c√¥te
- Informations de base : Run Name, Duration, Start Time

#### B. **Param√®tres (Parameters)**
- Comparez les hyperparam√®tres entre les runs :
  - `epochs` : Nombre d'√©poques d'entra√Ænement (3, 5, 10)
  - `imgsz` : Taille d'image (320, 416, 640)
  - `lr0` : Learning rate initial (0.001, 0.01, 0.1)
  - `seed` : Seed al√©atoire (42)
  - `model` : Architecture (yolov8n.pt)

#### C. **M√©triques (Metrics)**
Cliquez sur l'onglet **"Metrics"** pour voir :

| M√©trique | Description | Objectif |
|----------|-------------|----------|
| **metrics/precision(B)** | Pr√©cision des bo√Ætes de d√©tection | ‚¨ÜÔ∏è Plus √©lev√© = Meilleur |
| **metrics/recall(B)** | Rappel des bo√Ætes de d√©tection | ‚¨ÜÔ∏è Plus √©lev√© = Meilleur |
| **metrics/mAP50(B)** | mAP √† IoU 0.5 | ‚¨ÜÔ∏è Plus √©lev√© = Meilleur |
| **metrics/mAP50-95(B)** | mAP moyenn√© IoU 0.5-0.95 | ‚¨ÜÔ∏è Plus √©lev√© = Meilleur |

**Astuce** : Vous pouvez :
- Trier les colonnes pour identifier le meilleur run
- Cliquer sur les en-t√™tes pour changer l'ordre de tri
- S√©lectionner des m√©triques sp√©cifiques √† afficher

#### D. **Graphiques de M√©triques**
- Graphiques parall√®les montrant l'√©volution des m√©triques
- Possibilit√© de superposer les courbes pour comparer visuellement
- Utilisez la l√©gende pour identifier chaque run

### 5. Explorer les Artefacts

Pour chaque run, vous pouvez consulter les artefacts g√©n√©r√©s :

1. **Retournez √† la liste des runs** (cliquez sur "cv_yolo_tiny" dans la sidebar)
2. **Cliquez sur un nom de run** sp√©cifique pour ouvrir sa page de d√©tails
3. Faites d√©filer jusqu'√† la section **"Artifacts"**

#### Artefacts disponibles :

| Fichier | Description |
|---------|-------------|
| **results.png** | Graphiques d'entra√Ænement (loss, mAP, precision, recall par √©poque) |
| **confusion_matrix.png** | Matrice de confusion des pr√©dictions |
| **confusion_matrix_normalized.png** | Matrice de confusion normalis√©e |
| **F1_curve.png** | Courbe F1-score |
| **P_curve.png** | Courbe Precision-Confidence |
| **PR_curve.png** | Courbe Precision-Recall |
| **R_curve.png** | Courbe Recall-Confidence |
| **weights/best.pt** | Poids du meilleur mod√®le (t√©l√©chargeable) |
| **weights/last.pt** | Poids du dernier checkpoint |

**Pour t√©l√©charger un artefact** :
1. Cliquez sur le nom du fichier dans la section Artifacts
2. Le fichier s'ouvre dans le visualiseur MLflow (pour les images)
3. Utilisez le bouton de t√©l√©chargement pour sauvegarder localement

---

## üîç Analyse Recommand√©e

### 1. Identifier le Meilleur Run

Apr√®s avoir compar√© les runs, identifiez celui avec :
- ‚úÖ **mAP50-95 le plus √©lev√©** (m√©trique principale pour YOLO)
- ‚úÖ **Bon √©quilibre Precision/Recall**
- ‚úÖ **Temps d'entra√Ænement acceptable**

Exemple de crit√®res :
```
Meilleur Run :
- mAP50-95 > 0.25
- mAP50 > 0.30
- Precision > 0.40
- Recall > 0.70
```

### 2. Analyser l'Impact des Hyperparam√®tres

Comparez les runs pour comprendre l'impact de chaque param√®tre :

#### **Impact des √âpoques** (epochs: 3, 5, 10)
- Comparez des runs avec **m√™me imgsz et lr0** mais diff√©rentes √©poques
- Observer si plus d'√©poques = meilleur mAP (attention √† l'overfitting)

#### **Impact de la Taille d'Image** (imgsz: 320, 416, 640)
- Comparez des runs avec **m√™me epochs et lr0** mais diff√©rentes tailles
- Images plus grandes = g√©n√©ralement meilleur mAP mais temps plus long

#### **Impact du Learning Rate** (lr0: 0.001, 0.01, 0.1)
- Comparez des runs avec **m√™me epochs et imgsz** mais diff√©rents lr0
- Trouver le lr optimal (ni trop faible, ni trop √©lev√©)

### 3. Examiner les Courbes d'Entra√Ænement

Pour chaque run prometteur :

1. Ouvrez **results.png** :
   - V√©rifiez que les loss diminuent r√©guli√®rement
   - Assurez-vous que val/loss ne diverge pas de train/loss (overfitting)
   - Observez la convergence des m√©triques

2. Ouvrez **confusion_matrix.png** :
   - V√©rifiez la d√©tection correcte de la classe "person"
   - Identifiez les faux positifs/n√©gatifs

3. Ouvrez **PR_curve.png** :
   - Courbe Precision-Recall doit tendre vers le coin sup√©rieur droit
   - Surface sous la courbe = mAP

---

## üìä Interface MLflow : Navigation Rapide

### URLs Utiles

| Page | URL |
|------|-----|
| **Accueil MLflow** | http://localhost:5000 |
| **Exp√©rience cv_yolo_tiny** | http://localhost:5000/#/experiments/1 |
| **Comparaison de runs** | S√©lectionner runs ‚Üí Bouton "Compare" |

### Raccourcis Clavier (dans l'UI)

| Action | Raccourci |
|--------|-----------|
| Rechercher | `/` |
| S√©lectionner run | Clic sur checkbox |
| Ouvrir run | Clic sur nom du run |

---

## üéì Exemple Pratique : Comparaison de 3 Runs

### Configuration Exemple

Supposons que vous avez s√©lectionn√© ces 3 runs :

| Run | Epochs | ImgSz | LR | mAP50 | mAP50-95 | Precision | Recall |
|-----|--------|-------|-----|-------|----------|-----------|--------|
| **Run 1** | 3 | 320 | 0.01 | 0.32 | 0.27 | 0.45 | 0.77 |
| **Run 2** | 5 | 416 | 0.01 | 0.38 | 0.31 | 0.52 | 0.80 |
| **Run 3** | 10 | 640 | 0.001 | 0.41 | 0.34 | 0.58 | 0.82 |

### Analyse

1. **Meilleur Run : Run 3**
   - ‚úÖ mAP50-95 le plus √©lev√© (0.34)
   - ‚úÖ Meilleure pr√©cision (0.58)
   - ‚úÖ Meilleur recall (0.82)
   - ‚ö†Ô∏è Temps d'entra√Ænement plus long (10 √©poques, 640px)

2. **Compromis Vitesse/Performance : Run 2**
   - ‚úÖ Bonnes performances (mAP50-95 = 0.31)
   - ‚úÖ Temps d'entra√Ænement mod√©r√© (5 √©poques, 416px)
   - ‚úÖ Bon √©quilibre pour production

3. **Run Rapide : Run 1**
   - ‚úÖ Entra√Ænement rapide (3 √©poques, 320px)
   - ‚ö†Ô∏è Performances plus faibles
   - üí° Id√©al pour prototypage rapide

---

## üí° Conseils Pratiques

### ‚úÖ Bonnes Pratiques

1. **Comparez max 5 runs √† la fois** pour une lisibilit√© optimale
2. **Utilisez les filtres** pour s√©lectionner des runs similaires
3. **Notez vos observations** directement dans les tags MLflow
4. **T√©l√©chargez les artefacts** des meilleurs mod√®les pour r√©f√©rence

### ‚ö†Ô∏è Points d'Attention

1. **Seed** : Tous les runs utilisent seed=42, donc reproductibles
2. **Dataset** : Tiny COCO (60 images) = r√©sultats √† prendre avec pr√©caution
3. **Overfitting** : Surveillez val/loss vs train/loss dans results.png
4. **Temps** : Runs avec imgsz=640 et epochs=10 sont beaucoup plus lents

---

## üõ†Ô∏è D√©pannage

### Probl√®me : Bouton "Compare" n'appara√Æt pas
- **Solution** : Assurez-vous d'avoir s√©lectionn√© au moins 2 runs
- V√©rifiez que les checkboxes sont bien coch√©es

### Probl√®me : M√©triques manquantes
- **Solution** : V√©rifiez que le run s'est termin√© avec succ√®s
- Consultez les logs du run pour identifier les erreurs

### Probl√®me : Artefacts non visibles
- **Solution** : Attendez la fin compl√®te du run
- V√©rifiez le dossier `runs/` localement si n√©cessaire

---

## üìñ Ressources

- [Documentation MLflow - Comparing Runs](https://mlflow.org/docs/latest/tracking.html#comparing-runs)
- [Documentation Ultralytics - Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/)
- [mAP Explained](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)

---

**üéâ Vous savez maintenant comment analyser et comparer vos exp√©riences YOLO dans MLflow !**

Pour toute question, consultez votre instructeur ou la documentation MLflow.
